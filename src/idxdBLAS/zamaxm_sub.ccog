#include <stdlib.h>
#include <math.h>

#include "../config.h"
#include "../common/common.h"
#include "idxdBLAS.h"

/*[[[cog
import cog
import generate
import dataTypes
import amaxm
import vectorizations

code_block = generate.CodeBlock()
vectorizations.conditionally_include_vectorizations(code_block)
cog.out(str(code_block))
]]]*/
#ifdef __AVX__
  #include <immintrin.h>

#elif defined(__SSE2__)
  #include <emmintrin.h>

#else


#endif
//[[[end]]]

/**
 * @internal
 * @brief  Find maximum magnitude pairwise product between vectors of complex double precision
 *
 * Returns the magnitude of the pairwise product of maximum magnitude between X and Y.
 *
 * @param N vector length
 * @param X complex double precision vector
 * @param incX X vector stride (use every incX'th element)
 * @param Y complex double precision vector
 * @param incY Y vector stride (use every incY'th element)
 * @param amaxm scalar return
 *
 * @author Peter Ahrens
 * @date   15 Jan 2016
 */
void idxdBLAS_zamaxm_sub(const int N, const void *X, const int incX, const void *Y, const int incY, void *amaxm){
  const double *x = (const double*)X;
  const double *y = (const double*)Y;
  /*[[[cog
  cog.out(generate.generate(amaxm.AMaxM(dataTypes.DoubleComplex, "N", "x", "incX", "y", "incY", "amaxm"), cog.inFile, args, params, mode))
  ]]]*/
  #ifdef __AVX__
    __m256d abs_mask_tmp;
    {
      __m256d tmp;
      tmp = _mm256_set1_pd(1);
      abs_mask_tmp = _mm256_set1_pd(-1);
      abs_mask_tmp = _mm256_xor_pd(abs_mask_tmp, tmp);
      tmp = _mm256_cmp_pd(tmp, tmp, 0);
      abs_mask_tmp = _mm256_xor_pd(abs_mask_tmp, tmp);
    }
    double max_buffer_tmp[4] __attribute__((aligned(32))); (void)max_buffer_tmp;

    int i;

    __m256d x_0, x_1, x_2, x_3, x_4, x_5, x_6, x_7;
    __m256d y_0, y_1, y_2, y_3;
    __m256d m_0;
    m_0 = _mm256_setzero_pd();

    if(incX == 1 && incY == 1){

      for(i = 0; i + 8 <= N; i += 8, x += 16, y += 16){
        x_0 = _mm256_loadu_pd(((double*)x));
        x_1 = _mm256_loadu_pd(((double*)x) + 4);
        x_2 = _mm256_loadu_pd(((double*)x) + 8);
        x_3 = _mm256_loadu_pd(((double*)x) + 12);
        y_0 = _mm256_loadu_pd(((double*)y));
        y_1 = _mm256_loadu_pd(((double*)y) + 4);
        y_2 = _mm256_loadu_pd(((double*)y) + 8);
        y_3 = _mm256_loadu_pd(((double*)y) + 12);
        x_4 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_5 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_1, 0x5), _mm256_permute_pd(y_1, 0xF)), abs_mask_tmp);
        x_6 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_2, 0x5), _mm256_permute_pd(y_2, 0xF)), abs_mask_tmp);
        x_7 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_3, 0x5), _mm256_permute_pd(y_3, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm256_and_pd(_mm256_mul_pd(x_1, _mm256_permute_pd(y_1, 0x0)), abs_mask_tmp);
        x_2 = _mm256_and_pd(_mm256_mul_pd(x_2, _mm256_permute_pd(y_2, 0x0)), abs_mask_tmp);
        x_3 = _mm256_and_pd(_mm256_mul_pd(x_3, _mm256_permute_pd(y_3, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        m_0 = _mm256_max_pd(m_0, x_2);
        m_0 = _mm256_max_pd(m_0, x_3);
        m_0 = _mm256_max_pd(m_0, x_4);
        m_0 = _mm256_max_pd(m_0, x_5);
        m_0 = _mm256_max_pd(m_0, x_6);
        m_0 = _mm256_max_pd(m_0, x_7);
      }
      if(i + 4 <= N){
        x_0 = _mm256_loadu_pd(((double*)x));
        x_1 = _mm256_loadu_pd(((double*)x) + 4);
        y_0 = _mm256_loadu_pd(((double*)y));
        y_1 = _mm256_loadu_pd(((double*)y) + 4);
        x_2 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_3 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_1, 0x5), _mm256_permute_pd(y_1, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm256_and_pd(_mm256_mul_pd(x_1, _mm256_permute_pd(y_1, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        m_0 = _mm256_max_pd(m_0, x_2);
        m_0 = _mm256_max_pd(m_0, x_3);
        i += 4, x += 8, y += 8;
      }
      if(i + 2 <= N){
        x_0 = _mm256_loadu_pd(((double*)x));
        y_0 = _mm256_loadu_pd(((double*)y));
        x_1 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        i += 2, x += 4, y += 4;
      }
      if(i < N){
        x_0 = _mm256_set_pd(0, 0, ((double*)x)[1], ((double*)x)[0]);
        y_0 = _mm256_set_pd(0, 0, ((double*)y)[1], ((double*)y)[0]);
        x_1 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        x += ((N - i) * 2), y += ((N - i) * 2);
      }
    }else{

      for(i = 0; i + 8 <= N; i += 8, x += (incX * 16), y += (incY * 16)){
        x_0 = _mm256_set_pd(((double*)x)[((incX * 2) + 1)], ((double*)x)[(incX * 2)], ((double*)x)[1], ((double*)x)[0]);
        x_1 = _mm256_set_pd(((double*)x)[((incX * 6) + 1)], ((double*)x)[(incX * 6)], ((double*)x)[((incX * 4) + 1)], ((double*)x)[(incX * 4)]);
        x_2 = _mm256_set_pd(((double*)x)[((incX * 10) + 1)], ((double*)x)[(incX * 10)], ((double*)x)[((incX * 8) + 1)], ((double*)x)[(incX * 8)]);
        x_3 = _mm256_set_pd(((double*)x)[((incX * 14) + 1)], ((double*)x)[(incX * 14)], ((double*)x)[((incX * 12) + 1)], ((double*)x)[(incX * 12)]);
        y_0 = _mm256_set_pd(((double*)y)[((incY * 2) + 1)], ((double*)y)[(incY * 2)], ((double*)y)[1], ((double*)y)[0]);
        y_1 = _mm256_set_pd(((double*)y)[((incY * 6) + 1)], ((double*)y)[(incY * 6)], ((double*)y)[((incY * 4) + 1)], ((double*)y)[(incY * 4)]);
        y_2 = _mm256_set_pd(((double*)y)[((incY * 10) + 1)], ((double*)y)[(incY * 10)], ((double*)y)[((incY * 8) + 1)], ((double*)y)[(incY * 8)]);
        y_3 = _mm256_set_pd(((double*)y)[((incY * 14) + 1)], ((double*)y)[(incY * 14)], ((double*)y)[((incY * 12) + 1)], ((double*)y)[(incY * 12)]);
        x_4 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_5 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_1, 0x5), _mm256_permute_pd(y_1, 0xF)), abs_mask_tmp);
        x_6 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_2, 0x5), _mm256_permute_pd(y_2, 0xF)), abs_mask_tmp);
        x_7 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_3, 0x5), _mm256_permute_pd(y_3, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm256_and_pd(_mm256_mul_pd(x_1, _mm256_permute_pd(y_1, 0x0)), abs_mask_tmp);
        x_2 = _mm256_and_pd(_mm256_mul_pd(x_2, _mm256_permute_pd(y_2, 0x0)), abs_mask_tmp);
        x_3 = _mm256_and_pd(_mm256_mul_pd(x_3, _mm256_permute_pd(y_3, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        m_0 = _mm256_max_pd(m_0, x_2);
        m_0 = _mm256_max_pd(m_0, x_3);
        m_0 = _mm256_max_pd(m_0, x_4);
        m_0 = _mm256_max_pd(m_0, x_5);
        m_0 = _mm256_max_pd(m_0, x_6);
        m_0 = _mm256_max_pd(m_0, x_7);
      }
      if(i + 4 <= N){
        x_0 = _mm256_set_pd(((double*)x)[((incX * 2) + 1)], ((double*)x)[(incX * 2)], ((double*)x)[1], ((double*)x)[0]);
        x_1 = _mm256_set_pd(((double*)x)[((incX * 6) + 1)], ((double*)x)[(incX * 6)], ((double*)x)[((incX * 4) + 1)], ((double*)x)[(incX * 4)]);
        y_0 = _mm256_set_pd(((double*)y)[((incY * 2) + 1)], ((double*)y)[(incY * 2)], ((double*)y)[1], ((double*)y)[0]);
        y_1 = _mm256_set_pd(((double*)y)[((incY * 6) + 1)], ((double*)y)[(incY * 6)], ((double*)y)[((incY * 4) + 1)], ((double*)y)[(incY * 4)]);
        x_2 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_3 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_1, 0x5), _mm256_permute_pd(y_1, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm256_and_pd(_mm256_mul_pd(x_1, _mm256_permute_pd(y_1, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        m_0 = _mm256_max_pd(m_0, x_2);
        m_0 = _mm256_max_pd(m_0, x_3);
        i += 4, x += (incX * 8), y += (incY * 8);
      }
      if(i + 2 <= N){
        x_0 = _mm256_set_pd(((double*)x)[((incX * 2) + 1)], ((double*)x)[(incX * 2)], ((double*)x)[1], ((double*)x)[0]);
        y_0 = _mm256_set_pd(((double*)y)[((incY * 2) + 1)], ((double*)y)[(incY * 2)], ((double*)y)[1], ((double*)y)[0]);
        x_1 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        i += 2, x += (incX * 4), y += (incY * 4);
      }
      if(i < N){
        x_0 = _mm256_set_pd(0, 0, ((double*)x)[1], ((double*)x)[0]);
        y_0 = _mm256_set_pd(0, 0, ((double*)y)[1], ((double*)y)[0]);
        x_1 = _mm256_and_pd(_mm256_mul_pd(_mm256_permute_pd(x_0, 0x5), _mm256_permute_pd(y_0, 0xF)), abs_mask_tmp);
        x_0 = _mm256_and_pd(_mm256_mul_pd(x_0, _mm256_permute_pd(y_0, 0x0)), abs_mask_tmp);
        m_0 = _mm256_max_pd(m_0, x_0);
        m_0 = _mm256_max_pd(m_0, x_1);
        x += (incX * (N - i) * 2), y += (incY * (N - i) * 2);
      }
    }
    _mm256_store_pd(max_buffer_tmp, m_0);
    max_buffer_tmp[0] = (max_buffer_tmp[0] > max_buffer_tmp[2] ? max_buffer_tmp[0]: max_buffer_tmp[2]);
    max_buffer_tmp[1] = (max_buffer_tmp[1] > max_buffer_tmp[3] ? max_buffer_tmp[1]: max_buffer_tmp[3]);
    ((double*)amaxm)[0] = max_buffer_tmp[0];
    ((double*)amaxm)[1] = max_buffer_tmp[1];

  #elif defined(__SSE2__)
    __m128d abs_mask_tmp;
    {
      __m128d tmp;
      tmp = _mm_set1_pd(1);
      abs_mask_tmp = _mm_set1_pd(-1);
      abs_mask_tmp = _mm_xor_pd(abs_mask_tmp, tmp);
      tmp = _mm_cmpeq_pd(tmp, tmp);
      abs_mask_tmp = _mm_xor_pd(abs_mask_tmp, tmp);
    }
    double max_buffer_tmp[2] __attribute__((aligned(16))); (void)max_buffer_tmp;

    int i;

    __m128d x_0, x_1, x_2, x_3, x_4, x_5;
    __m128d y_0, y_1, y_2;
    __m128d m_0;
    m_0 = _mm_setzero_pd();

    if(incX == 1 && incY == 1){

      for(i = 0; i + 3 <= N; i += 3, x += 6, y += 6){
        x_0 = _mm_loadu_pd(((double*)x));
        x_1 = _mm_loadu_pd(((double*)x) + 2);
        x_2 = _mm_loadu_pd(((double*)x) + 4);
        y_0 = _mm_loadu_pd(((double*)y));
        y_1 = _mm_loadu_pd(((double*)y) + 2);
        y_2 = _mm_loadu_pd(((double*)y) + 4);
        x_3 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_0, x_0, 0x1), _mm_shuffle_pd(y_0, y_0, 0x3)), abs_mask_tmp);
        x_4 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_1, x_1, 0x1), _mm_shuffle_pd(y_1, y_1, 0x3)), abs_mask_tmp);
        x_5 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_2, x_2, 0x1), _mm_shuffle_pd(y_2, y_2, 0x3)), abs_mask_tmp);
        x_0 = _mm_and_pd(_mm_mul_pd(x_0, _mm_shuffle_pd(y_0, y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm_and_pd(_mm_mul_pd(x_1, _mm_shuffle_pd(y_1, y_1, 0x0)), abs_mask_tmp);
        x_2 = _mm_and_pd(_mm_mul_pd(x_2, _mm_shuffle_pd(y_2, y_2, 0x0)), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, x_0);
        m_0 = _mm_max_pd(m_0, x_1);
        m_0 = _mm_max_pd(m_0, x_2);
        m_0 = _mm_max_pd(m_0, x_3);
        m_0 = _mm_max_pd(m_0, x_4);
        m_0 = _mm_max_pd(m_0, x_5);
      }
      if(i + 2 <= N){
        x_0 = _mm_loadu_pd(((double*)x));
        x_1 = _mm_loadu_pd(((double*)x) + 2);
        y_0 = _mm_loadu_pd(((double*)y));
        y_1 = _mm_loadu_pd(((double*)y) + 2);
        x_2 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_0, x_0, 0x1), _mm_shuffle_pd(y_0, y_0, 0x3)), abs_mask_tmp);
        x_3 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_1, x_1, 0x1), _mm_shuffle_pd(y_1, y_1, 0x3)), abs_mask_tmp);
        x_0 = _mm_and_pd(_mm_mul_pd(x_0, _mm_shuffle_pd(y_0, y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm_and_pd(_mm_mul_pd(x_1, _mm_shuffle_pd(y_1, y_1, 0x0)), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, x_0);
        m_0 = _mm_max_pd(m_0, x_1);
        m_0 = _mm_max_pd(m_0, x_2);
        m_0 = _mm_max_pd(m_0, x_3);
        i += 2, x += 4, y += 4;
      }
      if(i + 1 <= N){
        x_0 = _mm_loadu_pd(((double*)x));
        y_0 = _mm_loadu_pd(((double*)y));
        x_1 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_0, x_0, 0x1), _mm_shuffle_pd(y_0, y_0, 0x3)), abs_mask_tmp);
        x_0 = _mm_and_pd(_mm_mul_pd(x_0, _mm_shuffle_pd(y_0, y_0, 0x0)), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, x_0);
        m_0 = _mm_max_pd(m_0, x_1);
        i += 1, x += 2, y += 2;
      }
    }else{

      for(i = 0; i + 3 <= N; i += 3, x += (incX * 6), y += (incY * 6)){
        x_0 = _mm_loadu_pd(((double*)x));
        x_1 = _mm_loadu_pd(((double*)x) + (incX * 2));
        x_2 = _mm_loadu_pd(((double*)x) + (incX * 4));
        y_0 = _mm_loadu_pd(((double*)y));
        y_1 = _mm_loadu_pd(((double*)y) + (incY * 2));
        y_2 = _mm_loadu_pd(((double*)y) + (incY * 4));
        x_3 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_0, x_0, 0x1), _mm_shuffle_pd(y_0, y_0, 0x3)), abs_mask_tmp);
        x_4 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_1, x_1, 0x1), _mm_shuffle_pd(y_1, y_1, 0x3)), abs_mask_tmp);
        x_5 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_2, x_2, 0x1), _mm_shuffle_pd(y_2, y_2, 0x3)), abs_mask_tmp);
        x_0 = _mm_and_pd(_mm_mul_pd(x_0, _mm_shuffle_pd(y_0, y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm_and_pd(_mm_mul_pd(x_1, _mm_shuffle_pd(y_1, y_1, 0x0)), abs_mask_tmp);
        x_2 = _mm_and_pd(_mm_mul_pd(x_2, _mm_shuffle_pd(y_2, y_2, 0x0)), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, x_0);
        m_0 = _mm_max_pd(m_0, x_1);
        m_0 = _mm_max_pd(m_0, x_2);
        m_0 = _mm_max_pd(m_0, x_3);
        m_0 = _mm_max_pd(m_0, x_4);
        m_0 = _mm_max_pd(m_0, x_5);
      }
      if(i + 2 <= N){
        x_0 = _mm_loadu_pd(((double*)x));
        x_1 = _mm_loadu_pd(((double*)x) + (incX * 2));
        y_0 = _mm_loadu_pd(((double*)y));
        y_1 = _mm_loadu_pd(((double*)y) + (incY * 2));
        x_2 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_0, x_0, 0x1), _mm_shuffle_pd(y_0, y_0, 0x3)), abs_mask_tmp);
        x_3 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_1, x_1, 0x1), _mm_shuffle_pd(y_1, y_1, 0x3)), abs_mask_tmp);
        x_0 = _mm_and_pd(_mm_mul_pd(x_0, _mm_shuffle_pd(y_0, y_0, 0x0)), abs_mask_tmp);
        x_1 = _mm_and_pd(_mm_mul_pd(x_1, _mm_shuffle_pd(y_1, y_1, 0x0)), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, x_0);
        m_0 = _mm_max_pd(m_0, x_1);
        m_0 = _mm_max_pd(m_0, x_2);
        m_0 = _mm_max_pd(m_0, x_3);
        i += 2, x += (incX * 4), y += (incY * 4);
      }
      if(i + 1 <= N){
        x_0 = _mm_loadu_pd(((double*)x));
        y_0 = _mm_loadu_pd(((double*)y));
        x_1 = _mm_and_pd(_mm_mul_pd(_mm_shuffle_pd(x_0, x_0, 0x1), _mm_shuffle_pd(y_0, y_0, 0x3)), abs_mask_tmp);
        x_0 = _mm_and_pd(_mm_mul_pd(x_0, _mm_shuffle_pd(y_0, y_0, 0x0)), abs_mask_tmp);
        m_0 = _mm_max_pd(m_0, x_0);
        m_0 = _mm_max_pd(m_0, x_1);
        i += 1, x += (incX * 2), y += (incY * 2);
      }
    }
    _mm_store_pd(max_buffer_tmp, m_0);
    ((double*)amaxm)[0] = max_buffer_tmp[0];
    ((double*)amaxm)[1] = max_buffer_tmp[1];

  #else
    int i;

    double x_0, x_1, x_2, x_3;
    double y_0, y_1;
    double m_0, m_1;
    m_0 = 0;
    m_1 = 0;

    if(incX == 1 && incY == 1){

      for(i = 0; i + 1 <= N; i += 1, x += 2, y += 2){
        x_0 = ((double*)x)[0];
        x_1 = ((double*)x)[1];
        y_0 = ((double*)y)[0];
        y_1 = ((double*)y)[1];
        x_2 = fabs(x_1 * y_1);
        x_3 = fabs(x_0 * y_1);
        x_0 = fabs(x_0 * y_0);
        x_1 = fabs(x_1 * y_0);
        m_0 = (m_0 > x_0? m_0: x_0);
        m_1 = (m_1 > x_1? m_1: x_1);
        m_0 = (m_0 > x_2? m_0: x_2);
        m_1 = (m_1 > x_3? m_1: x_3);
      }
    }else{

      for(i = 0; i + 1 <= N; i += 1, x += (incX * 2), y += (incY * 2)){
        x_0 = ((double*)x)[0];
        x_1 = ((double*)x)[1];
        y_0 = ((double*)y)[0];
        y_1 = ((double*)y)[1];
        x_2 = fabs(x_1 * y_1);
        x_3 = fabs(x_0 * y_1);
        x_0 = fabs(x_0 * y_0);
        x_1 = fabs(x_1 * y_0);
        m_0 = (m_0 > x_0? m_0: x_0);
        m_1 = (m_1 > x_1? m_1: x_1);
        m_0 = (m_0 > x_2? m_0: x_2);
        m_1 = (m_1 > x_3? m_1: x_3);
      }
    }
    ((double*)amaxm)[0] = m_0;
    ((double*)amaxm)[1] = m_1;

  #endif
  //[[[end]]]
}
